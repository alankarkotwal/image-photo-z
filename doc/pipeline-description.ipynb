{
 "metadata": {
  "name": "",
  "signature": "sha256:9da6bff8f978910e462c37e09d5de25831e57b773352661eeb9715b46bbfe6ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting to know the image-photo-z pipeline\n",
      "=================\n",
      "___\n",
      "\n",
      "_image-photo-z_ is a pixel-level method for estimating photometric redshifts. The process from beginning to end involves a number of transformations and procedures on the images that are documented here. Wherever and whenever possible, the logic, implementation, validation and expressions for the stages in the pipeline are provided.\n",
      "This file does **NOT** describe the API. See the API reference for more details regarding the API.\n",
      "\n",
      "## Getting the object catalog ready\n",
      "\n",
      "<p>For training the machine learning algorithm to be used, we need the pixel data and known redshifts from known sources. For this, a catalog of objects for which redshifts are known is necessary. This step gets this catalog from SDSS, which is our primary source of data.\n",
      "    We have followed a two-step procedure to achieve this. First, we query the SDSS server for all 'valid' objects. The meaning of 'valid' will be clear from the following SQL code we use (provided as image-photo-z/catalog_gen/sql/QueryDR10.sql, run on CasJobs):</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "SELECT TOP 10\n",
      "  spec.specObjID AS id,\n",
      "  spec.ra,\n",
      "  spec.dec,\n",
      "  spec.z AS redshift,\n",
      "  spec.zErr AS redshiftError,\n",
      "  specAll.class,\n",
      "  specAll.run,\n",
      "  specAll.camcol,\n",
      "  specAll.field,\n",
      "  specAll.dered_u AS u,\n",
      "  specAll.dered_g AS g,\n",
      "  specAll.dered_r AS r,\n",
      "  specAll.dered_i AS i,\n",
      "  specAll.dered_z AS z\n",
      "INTO objects_with_redshifts\n",
      "FROM SpecObj AS spec\n",
      "JOIN SpecPhotoAll AS specAll\n",
      "ON spec.specObjID = specAll.specObjID,\n",
      "  PhotoObj AS phot\n",
      "WHERE\n",
      "  specAll.ObjID = phot.ObjID\n",
      "  AND phot.CLEAN = 1\n",
      "  AND spec.zWarning = 0    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have selected from the objects with known redshifts some parameters (which are evident from their names) of these objects.\n",
      "We have taken data from only those objects which have clean photometry and no warnings in redshift measurement.\n",
      "\n",
      "Once we have data for all objects matching our criterion, we query this small database for a region of the sky we want to use as follows (provided as image-photo-z/catalog_gen/sql/GetArea.sql):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "SELECT ALL\n",
      "  objs.id,\n",
      "  objs.ra,\n",
      "  objs.dec,\n",
      "  objs.redshift,\n",
      "  objs.redshiftError,\n",
      "  objs.class,\n",
      "  objs.run,\n",
      "  objs.camcol,\n",
      "  objs.field\n",
      "INTO mydb.one_square_degree\n",
      "FROM mydb.objects_with_redshifts AS objs\n",
      "WHERE\n",
      "  objs.ra>180 AND objs.ra<181\n",
      "  AND objs.dec>45 AND objs.dec<46"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Insert in the above code appropriate values of ra and dec to get your desired region of the sky.\n",
      "\n",
      "We save the output of the above query as a CSV file and use this file as the input catalog for the pipeline."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Downloading images\n",
      "\n",
      "<p>The catalog files created above contain information on the run-camcol-field of the frame in which the above object was captured.\n",
      "This is the identity of the frame. We first enumerate all the distinct run-camcol-field combinations in a logfile so as to prevent multiple downloads of the same file.\n",
      "Now the URL of an image frame in the SDSS catalog is of the form http://data.sdss3.org/sas/dr10/boss/photoObj/frames/[rerun]/[run]/[camcol]/frame-[band]-[run]-[camcol]-[field].fits.bz2.\n",
      "We run this over the logfile, get images into the images folder and extract them.\n",
      "We name the images as [run]-[camcol]-[field]-[band].fits (3813-5-23-r.fits, for instance) for easy future reference.\n",
      "Note that if we choose to use local images, they need to be named similarly as well. The relevant code block is presented here for completeness.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "pref=\"http://data.sdss3.org/sas/dr10/boss/photoObj/frames/\"\n",
      "downloadURL=pref+rerun+\"/\"+run+\"/\"+camcol+\"/frame-\"+band+\"-\"+run.zfill(6)+\"-\"+\\\n",
      "camcol+\"-\"+field.zfill(4)+\".fits.bz2\"\n",
      "if not os.path.isfile(downloadFolder+\"/\"+run+\"-\"+camcol+\"-\"+field+\"-\"+band+\".fits\"):\n",
      "\tif band==bands[0]:\n",
      "\t\tlogfileFile.write(run+\"-\"+camcol+\"-\"+field+\"\\n\")\n",
      "\turllib.urlretrieve(downloadURL, downloadFolder+\"/\"+run+\"-\"+camcol+\"-\"+field+\\\n",
      "            \"-\"+band+\".fits.bz2\")\n",
      "\tos.system(\"bunzip2 \"+downloadFolder+\"/\"+run+\"-\"+camcol+\"-\"+field+\"-\"+band+\\\n",
      "            \".fits.bz2\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an additional feature, if an image is already present it is not downloaded again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting image errors\n",
      "\n",
      "Once we have the images, we have complete information about all pixels compressed in a FITS file.\n",
      "The primary HDU in the file is the actual corrected image of the sky (which corresponds to the pixel values we want). We do not need to put any effort at all to get the corrected frame values from the FITS file.\n",
      "However, some machine learning algorithms also require the errors in these pixel values. The 'error' values are encoded in a 'sky image matrix' as described in http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html.\n",
      "\n",
      "This 'sky image' is of size lesser than the actual image (256x192 vs 2048x1489) and we need to interpolate this image to \"blow\" it up at its right size.\n",
      "For this interpolation we use the scipy interp2d function. The relevant code block for getting the full error matrix is shown here:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "skyImageInit=fitsFile[2].data[0][0]\n",
      "# Since we're interpolating an image whose values are known at a grid, we use\n",
      "# interp2d. It requires an array of the grid x and grid y positions, which\n",
      "# is what we're generating in first two lines. Then we fit the interpolator.\n",
      "xs=numpy.fromfunction(lambda k: k, (skyImageInit.shape[0],), dtype=int) \n",
      "ys=numpy.fromfunction(lambda k: k, (skyImageInit.shape[1],), dtype=int) \n",
      "interpolator=interp2d(xs, ys, skyImageInit, kind='cubic')\n",
      "for j in range(errorImage.shape[1]):\n",
      "\tfor k in range(errorImage.shape[0]):\n",
      "\t\tskyImageValue=interpolator.__call__(j*skyImageInit.shape[0]\\\n",
      "                /errorImage.shape[0],k*skyImageInit.shape[1]/errorImage.shape[1])\n",
      "\t\terrorImage[k][j]=return_sdss_pixelError(band, camcol, run,\\\n",
      "                fitsImage[k][j], skyImageValue, errorImg[k][j])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we have fitted a function to the smaller 256x192 array using a cubic interpolation and to 'expand' it, have scaled the axes by (1489/256) and (2048/192) respectively.\n",
      "\n",
      "The errors are calculated as shown in the link above. The relevant functions are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def return_sdss_gain(band, camcol, run):\n",
      "\tlut=[{},{'u':1.62,'g':3.32,'r':4.71,'i':5.165,'z':4.745},\\\n",
      "         {'u':1.595,'g':3.855,'r':4.6,'i':6.565,'z':5.155},\\\n",
      "         {'u':1.59,'g':3.845,'r':4.72,'i':4.86,'z':4.885},\\\n",
      "         {'u':1.6,'g':3.995,'r':4.76,'i':4.885,'z':4.775},\\\n",
      "         {'u':1.47,'g':4.05,'r':4.725,'i':4.64,'z':3.48},\\\n",
      "         {'u':2.17,'g':4.035,'r':4.895,'i':4.76,'z':4.69}]\n",
      "\tif run>1100:\n",
      "\t\tlut[2]['u']=1.825\n",
      "\treturn lut[camcol][band]\n",
      "\n",
      "\n",
      "def return_sdss_darkVariance(band, camcol, run):\n",
      "\tlut=[{},{'u':9.61,'g':15.6025,'r':1.8225,'i':7.84,'z':0.81},\\\n",
      "         {'u':12.6025,'g':1.44,'r':1.0,'i':5.76,'z':1.0},\\\n",
      "         {'u':8.7025,'g':1.3225,'r':1.3225,'i':4.6225,'z':1.0},\\\n",
      "         {'u':12.6025,'g':1.96,'r':1.3225,'i':6.25,'z':9.61},\\\n",
      "         {'u':9.3025,'g':1.1025,'r':0.81,'i':7.84,'z':1.8225},\\\n",
      "         {'u':7.0225,'g':1.8225,'r':0.9025,'i':5.0625,'z':1.21}]\n",
      "\tif run>1500:\n",
      "\t\tlut[2]['i']=6.25\n",
      "\t\tlut[4]['i']=7.5625\n",
      "\t\tlut[4]['z']=12.6025\n",
      "\t\tlut[5]['z']=2.1025\n",
      "\treturn lut[camcol][band]\n",
      "\n",
      "\n",
      "def return_sdss_pixelError(band, camcol, run, img, simg, cimg):\n",
      "\tdn=img/cimg+simg\n",
      "\tdn_err=math.sqrt(abs((dn/return_sdss_gain(band, camcol, run))+\\\n",
      "            return_sdss_darkVariance(band, camcol, run)))\n",
      "\timg_err=dn_err*cimg\n",
      "\treturn img_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This was validated in the following way:\n",
      "\n",
      "1) Checked registration between original error and original image.  \n",
      "2) Checked registration between registered error and registered image.\n",
      "\n",
      "These images match well in registration. Now the error predicted by the SDSS model increases with increasing image intensities.\n",
      "Since bright regions seem to match between images, astrometry seems to be fine between these images.\n",
      "\n",
      "3) Used ds9 to check error magnitudes with respect to signal value.\n",
      "The error seemed to be around the signal value in the background and significantly less than the signal at object centers.\n",
      "At object boundaries though, sometimes errors become greater than signal values (as expected)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Image Registration\n",
      "\n",
      "Now we have images and image errors in all filters. We need to align images in different filters to make sure corresponding pixels line up.\n",
      "This process is Image Registration. We implement this using the Montage Image Mosaic Engine and montage_wrapper for Python. The relevant code lines are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "montage_wrapper.commands.mGetHdr(refImage,headerName)\n",
      "montage_wrapper.reproject(inputImages,outputImages,header=headerName,\\\n",
      "            north_aligned=True,system='EQUJ',exact_size=True,common=True,...)\n",
      "montage_wrapper.reproject(intErrorImages,outputErrorImages,header=headerName,\\\n",
      "            north_aligned=True,system='EQUJ',exact_size=True,common=True,...)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reproject function reprojects the given image list to a given FITS header. If some images have the same header, they naturally have the same astrometry and hence are aligned.\n",
      "So we reproject all images to the same (reference image) header and get the aligned output images. The 'exact_size' argument must be set to true for images to align exactly.\n",
      "\n",
      "We validated this by using the 'blink' feature in ds9 FITS viewer. One can visually observe that the images are well-aligned after following this procedure by flipping through them using this feature on ds9."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generating pixel data\n",
      "\n",
      "We are now ready to generate pixel data. The final output from this stage needs to be vectors of the form ([list of magnitudes in all bands], [redshift, redshiftError], [other optional parameters]) corresponding to each pixel.\n",
      "Redshifts are assigned to pixels from the catalogs by recognizing which object the pixel belongs to. This is implemented using ASSOC in SExtractor.\n",
      "SExtractor is run on images in all bands for a frame with the standard (for now) set of parameters.\n",
      "\n",
      "The ASSOC requires that we supply to it a list of expected pixel coordinates for objects. It returns to us a list of parameters we specify in the config file.\n",
      "The conversion from absolute object coordinates to pixel coordinates for the given frame, given the image header is done using astropy.wcs as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Get the FITS hdulist\n",
      "hdulist = fits.open(filename)\n",
      "# Create the coordinate system object using the header\n",
      "w = wcs.WCS(hdulist[0].header)\n",
      "# Calculate expected pixel coordinates\n",
      "px, py = w.wcs_world2pix([ra], [dec], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The part of the config file related to the ASSOC is (the significance of this can be found in the SExtractor manual):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "ASSOC_NAME       sky.list       # name of the ASCII file to ASSOCiate, the expected pixel \n",
      "                                # coordinates list given as [id, xpos, ypos]\n",
      "ASSOC_DATA       1              # columns of the data to replicate (0=all), replicate id\n",
      "                                # of the object in the SExtractor output file\n",
      "ASSOC_PARAMS     2,3            # columns of xpos,ypos[,mag] in the expected pixel\n",
      "                                # coordinates list\n",
      "ASSOC_RADIUS     5.0            # cross-matching radius (pixels)\n",
      "ASSOC_TYPE       NEAREST        # ASSOCiation method: FIRST, NEAREST, MEAN,\n",
      "                                # MAG_MEAN, SUM, MAG_SUM, MIN or MAX\n",
      "ASSOCSELEC_TYPE  MATCHED        # ASSOC selection type: ALL, MATCHED or -MATCHED"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also ask SExtractor for the peak (in intensity), maximum and minimum pixel coordinates from each identified object and a segmentation image (which is non-zero only wherever an object is found, and is a particular \"object mask\" value wherever an object is found. This object mask is unique to a given object).\n",
      "\n",
      "Now given all this, for a given object, we note the object mask for its peak pixel. All pixels having this mask belong to this object. We scan the array from minimum to maximum coordinates and add the pixel magnitudes if the object masks match.\n",
      "We also remove those pixels whose intensities are nan in the way.\n",
      "The relevant part of the code is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Iterate over all pixels in the rectangle\n",
      "for j in range([minObjectX], [maxObjectX]):\n",
      "\tfor k in range([minObjectY], [maxObjectY]):\n",
      "\t\tisPixelValid=1\n",
      "\t\tfor l in fitsImages:\n",
      "                    # Make pixel invalid if any of the values is a nan\n",
      "\t\t\tif math.isnan(float(l[k][j])):\n",
      "\t\t\t\tisPixelValid=0\n",
      "\t\t\tif isPixelValid==1:\n",
      "                        # Check if the pixel belongs to our object\n",
      "                        # The object flag has been noted earlier in the code\n",
      "\t\t\t\tif segImage[k][j]==thisObjFlag:\n",
      "\t\t\t\t\ttrainingVector=[]\n",
      "\t\t\t\t\tfor l in fitsImages:\n",
      "\t\t\t\t\t\ttrainingVector.append(float(l[k][j]))\n",
      "\t\t\t\t\ttrainingVector.append(redshift)\n",
      "\t\t\t\t\ttrainingVector.append(redshiftError)\n",
      "\t\t\t\t\ttrainingVector.append(objClass)\n",
      "                                        pixelRA, pixelDec = w.wcs_pix2world(float(j),\\\n",
      "                                                float(k), 1)\n",
      "                                        trainingVector.append(pixelRA)\n",
      "                                        trainingVector.append(pixelDec)\n",
      "                                        distance=math.sqrt(pow((k-thisObjX),2)+\\\n",
      "                                                pow((j-thisObjY),2))/maxDist\n",
      "                                        trainingVector.append(distance)\n",
      "                                        for l in fitsErrors:\n",
      "\t\t\t\t\t\ttrainingVector.append(float(l[k][j]))\n",
      "                        # Build the training array for this object\n",
      "\t\t\t\t\ttrainingArray.append(trainingVector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training and testing\n",
      "\n",
      "Now that we have pixel data, we train and test some machine learning algorithms. Currently, two algorithms are implemented in the code: kNN and MLZ.\n",
      "kNN is implemented with Scikit-learn.\n",
      "First we get the classified data ready into the file format that these software packages require the data to be in for training. Refer to the documentation for MLZ and Scikit-learn kNNClassifier and kNNRegressor for more details."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Visualising output\n",
      "\n",
      "If using MLZ, consult the MLZ documentation for a description of how to generate plots and so on.\n",
      "\n",
      "If using kNN, the kNN output file specified in the config can be plotted directly on GNUPlot to obtain a graph of predicted versus actual redshift or class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Further improvements\n",
      "\n",
      "This is a work under progress, and further improvements and fine-tuning must be made. At this stage, this is a rough to-do list:\n",
      "\n",
      "1) Check objects that give isolated pixels. In the segmentation maps generated by SExtractor, some segments seem to give pixels that are isolated\n",
      "   from the object itself. Check why this is happening and see if changing SExtractor input parameters improves this.\n",
      "\n",
      "2) Some pixels seem to give negative flux values. This also happens in the images themselves, so this is not explicitly a problem with the code.\n",
      "   Check if this is valid for the training.\n",
      "\n",
      "3) Check why the image registration part takes a lot of time and remove the bottleneck. The \"blowing up\" of the sky noise image from the FITS HDU\n",
      "   takes a lot of time. Check this.\n",
      "\n",
      "4) Remove duplicate pixels between images. Some images in the data-set overlap and this is necessary.\n",
      "\n",
      "5) Port the code to run on a GPU.\n",
      "\n",
      "6) Automate generation of the catalog using sdssCL.\n",
      "\n",
      "7) Add timing support.\n",
      "\n",
      "8) Implement other machine learning algorithms.\n",
      "\n",
      "9) Check the validity of the interpolation used to \"blow up\" the sky image. Use other interpolations if more valid.\n",
      "\n",
      "10) Check how pixel redshifts are combined to form object redshifts. For now, we're doing direct averaging.\n",
      "\n",
      "11) Create a setup file or an installer to install the package in some standard location like dist-utils.\n",
      "\n",
      "12) Add more stuff to this file."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}